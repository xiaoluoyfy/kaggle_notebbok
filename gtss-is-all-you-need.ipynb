{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013796,
     "end_time": "2022-03-31T13:23:37.784737",
     "exception": false,
     "start_time": "2022-03-31T13:23:37.770941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# How to validate a model on chronologically ordered data which also contains groups?\n",
    "\n",
    "Since it takes quite some time to get a utility (leaderboard) score back for our model, it would be nice to be able to 'locally' calculate an indication of a model's performance; independent of the (time expensive and limited) submission API. This would allow for much better tuning of hyper-parameters or other aspects of the model's training process.\n",
    "\n",
    "In this notebook I want to lay out a couple of techniques that can be used to do this. For every step we will see that there is a problem with using it for this particular competition. Fortunately the last chapter provides a solution! If you are not interested in an introduction in test and validation techniques, then skip to the bottom. First up: train and test subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010313,
     "end_time": "2022-03-31T13:23:37.805865",
     "exception": false,
     "start_time": "2022-03-31T13:23:37.795552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train and Test Subsets\n",
    "\n",
    "The first obvious step is to set apart some data which the model never gets to see. After the model has been trained, we use that unseen data to verify our model's predicitions. Scikit-learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) makes the process of splitting datasets easy for us. Let's load our training data and set aside a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:37.830220Z",
     "iopub.status.busy": "2022-03-31T13:23:37.829618Z",
     "iopub.status.idle": "2022-03-31T13:23:42.571299Z",
     "shell.execute_reply": "2022-03-31T13:23:42.570074Z",
     "shell.execute_reply.started": "2022-03-31T13:18:34.717616Z"
    },
    "papermill": {
     "duration": 4.754923,
     "end_time": "2022-03-31T13:23:42.571469",
     "exception": false,
     "start_time": "2022-03-31T13:23:37.816546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    dtrain = pd.read_parquet('../input/found-the-holy-grail-grouptimeseriessplit/dtrain.parquet')\n",
    "except:\n",
    "    dtrain = pd.read_csv('../input/jane-street-market-prediction/train.csv', index_col='ts_id')\n",
    "    dtrain = dtrain.astype({c: np.float32 for c, t in dtrain.dtypes.items() if t == np.float64})\n",
    "    #dtrain.to_parquet('dtrain.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:43.144614Z",
     "iopub.status.busy": "2022-03-31T13:23:43.143963Z",
     "iopub.status.idle": "2022-03-31T13:23:43.149027Z",
     "shell.execute_reply": "2022-03-31T13:23:43.149475Z",
     "shell.execute_reply.started": "2022-03-31T13:18:40.187714Z"
    },
    "papermill": {
     "duration": 0.566688,
     "end_time": "2022-03-31T13:23:43.149632",
     "exception": false,
     "start_time": "2022-03-31T13:23:42.582944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'weight', 'feature_0', 'feature_1', 'feature_2', 'feature_3',\n",
      "       'feature_4', 'feature_5', 'feature_6', 'feature_7',\n",
      "       ...\n",
      "       'feature_120', 'feature_121', 'feature_122', 'feature_123',\n",
      "       'feature_124', 'feature_125', 'feature_126', 'feature_127',\n",
      "       'feature_128', 'feature_129'],\n",
      "      dtype='object', length=132)\n",
      "Index(['resp_1', 'resp_2', 'resp_3', 'resp_4', 'resp'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dlabels = dtrain.filter(regex='resp')\n",
    "dtrain = dtrain.drop(dlabels.columns, axis=1)\n",
    "\n",
    "print(dtrain.columns)\n",
    "print(dlabels.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:43.177945Z",
     "iopub.status.busy": "2022-03-31T13:23:43.177156Z",
     "iopub.status.idle": "2022-03-31T13:23:45.013031Z",
     "shell.execute_reply": "2022-03-31T13:23:45.013577Z",
     "shell.execute_reply.started": "2022-03-31T13:18:41.325755Z"
    },
    "papermill": {
     "duration": 1.852806,
     "end_time": "2022-03-31T13:23:45.013727",
     "exception": false,
     "start_time": "2022-03-31T13:23:43.160921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1792868, 132) (597623, 132)\n",
      "Int64Index([      0,       1,       2,       3,       4,       5,       6,\n",
      "                  7,       8,       9,\n",
      "            ...\n",
      "            1792858, 1792859, 1792860, 1792861, 1792862, 1792863, 1792864,\n",
      "            1792865, 1792866, 1792867],\n",
      "           dtype='int64', name='ts_id', length=1792868)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    dtrain,\n",
    "    dlabels,\n",
    "    test_size=.25,\n",
    "    random_state=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(x_train.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011162,
     "end_time": "2022-03-31T13:23:45.036490",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.025328",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Use of `shuffle=False` is key here; since otherwise we would lose all chronological order.\n",
    "\n",
    "However, this approach is problematic because by constantly verifying on the same data, we also slowly start to overfit on the test set (\"leakage\"). Splitting the test set again into a validation set could solve this: the model's hyper-parameters are tuned and verified on the validation set and once that is completely finished we test it (only once!) on the test set. The problem now becomes that either the test and validation sets become too small to be useful or that so much data is used to validate and test that nog enough data remains to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0112,
     "end_time": "2022-03-31T13:23:45.059276",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.048076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Cross-validation\n",
    "\n",
    "In cross-validation (CV), multiple validation sets are derived from the training set. Every *fold* a new part of the training set is used as the vaildation set, and the data previously used for validation now becomes part of the training set again:\n",
    "\n",
    "![img](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "(Source: [scikit-learn's User Guide, Ch. 3.1](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:45.085574Z",
     "iopub.status.busy": "2022-03-31T13:23:45.084955Z",
     "iopub.status.idle": "2022-03-31T13:23:45.314374Z",
     "shell.execute_reply": "2022-03-31T13:23:45.315104Z",
     "shell.execute_reply.started": "2022-03-31T13:18:42.146376Z"
    },
    "papermill": {
     "duration": 0.244346,
     "end_time": "2022-03-31T13:23:45.315327",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.070981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 358574,  358575,  358576,  358577,  358578,  358579,  358580,\n",
      "             358581,  358582,  358583,\n",
      "            ...\n",
      "            1792858, 1792859, 1792860, 1792861, 1792862, 1792863, 1792864,\n",
      "            1792865, 1792866, 1792867],\n",
      "           dtype='int64', name='ts_id', length=1434294)\n",
      "Int64Index([     0,      1,      2,      3,      4,      5,      6,      7,\n",
      "                 8,      9,\n",
      "            ...\n",
      "            358564, 358565, 358566, 358567, 358568, 358569, 358570, 358571,\n",
      "            358572, 358573],\n",
      "           dtype='int64', name='ts_id', length=358574)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "for train_idx, test_idx in KFold().split(x_train):\n",
    "    #print(train, test)\n",
    "    print(x_train.loc[train_idx, 'date'].index)\n",
    "    print(x_train.loc[test_idx, 'date'].index)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011607,
     "end_time": "2022-03-31T13:23:45.339394",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.327787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TimeSeriesSplit\n",
    "\n",
    "Note however, that in the first split shown above we are validating our *chronological* data on the past. We are training on trades starting from `358574` but testing on trades starting from `0`. In other words, our model has been trained using information which wasn't yet available at the time of the validation set. This is clear leakage; we are predicting the past with knowledge from the future. But our aim is to predict data in the future! This problem has already been addressed by scikit-learn in the form of [TimeSeriesSplit](https://scikit-learn.org/stable/modules/cross_validation.html#time-series-split):\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0101.png)\n",
    "But what is the problem this time? `TimeSeriesSplit` does not respect the groups available in the data. Although not clearly visible in this plot, we can imagine that a group can partially fall in the training set and partially in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:45.368930Z",
     "iopub.status.busy": "2022-03-31T13:23:45.368320Z",
     "iopub.status.idle": "2022-03-31T13:23:45.731107Z",
     "shell.execute_reply": "2022-03-31T13:23:45.731593Z",
     "shell.execute_reply.started": "2022-03-31T13:18:42.447510Z"
    },
    "papermill": {
     "duration": 0.380486,
     "end_time": "2022-03-31T13:23:45.731733",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.351247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
      "[ 44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61\n",
      "  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
      "  98  99 100 101 102 103 104]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104]\n",
      "[104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175\n",
      " 176 177 178 179 180 181 182 183 184]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184]\n",
      "[184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201\n",
      " 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254]\n",
      "[254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
      " 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289\n",
      " 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307\n",
      " 308 309 310 311 312 313 314 315 316 317 318 319 320]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320]\n",
      "[320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "for train_idx, test_idx in TimeSeriesSplit(n_splits=5).split(x_train):\n",
    "    print(x_train.loc[train_idx, 'date'].unique())\n",
    "    print(x_train.loc[test_idx, 'date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012871,
     "end_time": "2022-03-31T13:23:45.757422",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.744551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Already in the first split we can see data from day `44` present in both the training and test set. That would mean that we are training on half of the trades of a certain day, just to validate their performance on the other half of the trades of that day. What we of course want is to train on all trades of a particular day, and to validate them on the day that follows! Otherwise again leaking will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012678,
     "end_time": "2022-03-31T13:23:45.784030",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.771352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### GroupKFold\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_indices_0051.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:45.813786Z",
     "iopub.status.busy": "2022-03-31T13:23:45.812791Z",
     "iopub.status.idle": "2022-03-31T13:23:46.031837Z",
     "shell.execute_reply": "2022-03-31T13:23:46.032337Z",
     "shell.execute_reply.started": "2022-03-31T13:18:42.872868Z"
    },
    "papermill": {
     "duration": 0.235619,
     "end_time": "2022-03-31T13:23:46.032491",
     "exception": false,
     "start_time": "2022-03-31T13:23:45.796872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   1   2   3   4   9  10  11  12  13  14  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  29  30  32  33  34  35  36  37  38  39  40  41\n",
      "  42  43  45  47  48  49  50  52  53  54  55  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  68  69  71  72  73  75  76  77  78  79  82  83  84\n",
      "  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 116 117 121 122 123 124 125 126\n",
      " 127 129 130 131 132 134 135 136 137 138 139 140 141 142 143 144 147 148\n",
      " 149 150 151 152 153 154 155 156 158 159 160 161 162 163 164 165 166 168\n",
      " 169 171 173 174 175 176 178 179 180 181 182 183 184 185 188 190 191 192\n",
      " 193 194 195 196 199 200 202 203 206 208 209 210 211 212 213 214 215 216\n",
      " 217 219 220 221 222 223 226 227 230 231 232 233 235 236 237 238 239 240\n",
      " 241 244 248 249 250 251 252 253 254 255 256 258 259 260 261 263 265 266\n",
      " 268 269 270 271 272 273 274 275 276 278 279 280 282 283 284 286 287 288\n",
      " 289 290 291 292 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 318 321 322 323 324 325 327 328 330 331 333 335\n",
      " 336 337 338 339 340 341 342 343 345 347 348 350 351 353 356 358 359 360\n",
      " 362 364 365 366 367 368 370 371 372 373 374 375 376 377 378 379 380 381]\n",
      "[  5   6   7   8  28  31  44  46  51  70  74  80  81  94 114 115 118 119\n",
      " 120 128 133 145 146 157 167 170 172 177 186 187 189 197 198 201 204 205\n",
      " 207 218 224 225 228 229 234 242 243 245 246 247 257 262 264 267 277 281\n",
      " 285 293 294 295 296 317 319 320 326 329 332 334 344 346 349 352 354 355\n",
      " 357 361 363 369 382]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "for train_idx, test_idx in GroupKFold().split(x_train, groups=x_train['date']):\n",
    "    print(x_train.loc[train_idx, 'date'].unique())\n",
    "    print(x_train.loc[test_idx, 'date'].unique())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012727,
     "end_time": "2022-03-31T13:23:46.058266",
     "exception": false,
     "start_time": "2022-03-31T13:23:46.045539",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The [GroupKFold](https://scikit-learn.org/stable/modules/cross_validation.html#group-k-fold) iterator does respect groupings: no group will ever be part of two folds. Unfortunately, it is also clear that it mixes up the order completely and thus loses the temporal dimension again. What we need is a a crossover between `GroupKFold` and `TimeSeriesSplit`: `GroupTimesSeriesSplit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012673,
     "end_time": "2022-03-31T13:23:46.083974",
     "exception": false,
     "start_time": "2022-03-31T13:23:46.071301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### GroupTimesSeriesSplit\n",
    "\n",
    "OK, so this iterator does not exist yet in scikit-learn. However, a request for it has been documented on GitHub over a year ago ([Feature request: Group aware Time-based cross validation #14257](https://github.com/scikit-learn/scikit-learn/issues/14257)) and is almost ready for release. Thanks to open source we can take a sneak peek already! \n",
    "\n",
    "**Do note that this is not fully reviewed yet!!!** This might be the final code that it will make it into `sklearn`'s version `0.24` as a major feature, but there's also a chance of bugs still being present.\n",
    "\n",
    "I did not write *any* of this but it did take me a good day of research and trying to write it myself. All credits go to [@getgaurav2](https://github.com/getgaurav2/).\n",
    "\n",
    "Here are some more attempts at grouped cross-validation I encountered in my research:\n",
    "- https://stackoverflow.com/questions/51963713/cross-validation-for-grouped-time-series-panel-data\n",
    "- https://datascience.stackexchange.com/questions/77684/time-series-grouped-cross-validation\n",
    "- https://nander.cc/writing-custom-cross-validation-methods-grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:46.129772Z",
     "iopub.status.busy": "2022-03-31T13:23:46.128767Z",
     "iopub.status.idle": "2022-03-31T13:23:46.131870Z",
     "shell.execute_reply": "2022-03-31T13:23:46.131300Z",
     "shell.execute_reply.started": "2022-03-31T13:18:43.104997Z"
    },
    "papermill": {
     "duration": 0.034888,
     "end_time": "2022-03-31T13:23:46.131976",
     "exception": false,
     "start_time": "2022-03-31T13:23:46.097088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class GroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_size : int, default=None\n",
    "        Maximum size for a single training set.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.model_selection import GroupTimeSeriesSplit\n",
    "    >>> groups = np.array(['a', 'a', 'a', 'a', 'a', 'a',\\\n",
    "                           'b', 'b', 'b', 'b', 'b',\\\n",
    "                           'c', 'c', 'c', 'c',\\\n",
    "                           'd', 'd', 'd'])\n",
    "    >>> gtss = GroupTimeSeriesSplit(n_splits=3)\n",
    "    >>> for train_idx, test_idx in gtss.split(groups, groups=groups):\n",
    "    ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
    "    ...     print(\"TRAIN GROUP:\", groups[train_idx],\\\n",
    "                  \"TEST GROUP:\", groups[test_idx])\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5] TEST: [6, 7, 8, 9, 10]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a']\\\n",
    "    TEST GROUP: ['b' 'b' 'b' 'b' 'b']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] TEST: [11, 12, 13, 14]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b']\\\n",
    "    TEST GROUP: ['c' 'c' 'c' 'c']\n",
    "    TRAIN: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\\\n",
    "    TEST: [15, 16, 17]\n",
    "    TRAIN GROUP: ['a' 'a' 'a' 'a' 'a' 'a' 'b' 'b' 'b' 'b' 'b' 'c' 'c' 'c' 'c']\\\n",
    "    TEST GROUP: ['d' 'd' 'd']\n",
    "    \"\"\"\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_size=None\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_size = max_train_size\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "        group_test_size = n_groups // n_folds\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "            for train_group_idx in unique_groups[:group_test_start]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "            train_end = train_array.size\n",
    "            if self.max_train_size and self.max_train_size < train_end:\n",
    "                train_array = train_array[train_end -\n",
    "                                          self.max_train_size:train_end]\n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:23:46.166704Z",
     "iopub.status.busy": "2022-03-31T13:23:46.165294Z",
     "iopub.status.idle": "2022-03-31T13:24:40.264648Z",
     "shell.execute_reply": "2022-03-31T13:24:40.263796Z",
     "shell.execute_reply.started": "2022-03-31T13:18:43.123814Z"
    },
    "papermill": {
     "duration": 54.119507,
     "end_time": "2022-03-31T13:24:40.264784",
     "exception": false,
     "start_time": "2022-03-31T13:23:46.145277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67]\n",
      "[ 68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130]\n",
      "[131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188 189 190 191 192 193]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193]\n",
      "[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229\n",
      " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255 256]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256]\n",
      "[257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319]\n",
      "[320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382]\n"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in GroupTimeSeriesSplit().split(x_train, groups=x_train['date']):\n",
    "    print(x_train.loc[train_idx, 'date'].unique())\n",
    "    print(x_train.loc[test_idx, 'date'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015824,
     "end_time": "2022-03-31T13:24:40.296885",
     "exception": false,
     "start_time": "2022-03-31T13:24:40.281061",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Chronological order is maintained and group separation is respected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:24:40.355115Z",
     "iopub.status.busy": "2022-03-31T13:24:40.354218Z",
     "iopub.status.idle": "2022-03-31T13:24:40.370687Z",
     "shell.execute_reply": "2022-03-31T13:24:40.370091Z",
     "shell.execute_reply.started": "2022-03-31T13:19:37.430176Z"
    },
    "papermill": {
     "duration": 0.057446,
     "end_time": "2022-03-31T13:24:40.370804",
     "exception": false,
     "start_time": "2022-03-31T13:24:40.313358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T13:24:40.409291Z",
     "iopub.status.busy": "2022-03-31T13:24:40.408612Z",
     "iopub.status.idle": "2022-03-31T13:25:29.705577Z",
     "shell.execute_reply": "2022-03-31T13:25:29.704931Z",
     "shell.execute_reply.started": "2022-03-31T13:19:37.469679Z"
    },
    "papermill": {
     "duration": 49.318988,
     "end_time": "2022-03-31T13:25:29.705699",
     "exception": false,
     "start_time": "2022-03-31T13:24:40.386711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36]\n",
      "[ 68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148\n",
      " 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184\n",
      " 185 186 187 188 189 190 191 192 193]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162]\n",
      "[194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229\n",
      " 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 250 251 252 253 254 255 256]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225]\n",
      "[257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274\n",
      " 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292\n",
      " 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288]\n",
      "[320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355\n",
      " 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382]\n"
     ]
    }
   ],
   "source": [
    "for train_idx, test_idx in PurgedGroupTimeSeriesSplit(n_splits = 5, group_gap = 31).split(x_train, groups=x_train['date']):\n",
    "    print(x_train.loc[train_idx, 'date'].unique())\n",
    "    print(x_train.loc[test_idx, 'date'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016151,
     "end_time": "2022-03-31T13:25:29.737709",
     "exception": false,
     "start_time": "2022-03-31T13:25:29.721558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 116.317159,
   "end_time": "2022-03-31T13:25:29.861326",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-31T13:23:33.544167",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
